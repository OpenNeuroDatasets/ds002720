0. Sections
------------

1. Project
2. Dataset
3. Terms of Use
4. Contents
5. Method and Processing

1. PROJECT
------------

Title: Brain-Computer Music Interface for Monitoring and Inducing Affective States (BCMI-MIdAS)
Dates: 2012-2017
Funding organisation: Engineering and Physical Sciences Research Council (EPSRC)
Grant no.: EP/J003077/1

2. DATASET
------------

Title: EEG from a Brain-Computer Music Interface for controlling music tempo.

Description: This dataset accompanies the publication by Daly et al. (2018) and has been analysed in Daly et al. (2014a; 2014b) (please see Section 5 for full references). The dataset is obtained from a music-based Brain-Computer Interface constructed to allow users to modulate the tempo of a piece of music dynamically via intentional control. The dataset contains the electroencephalogram (EEG) data from 19 healthy participants instructed to increase the tempo of the music via kinaesthetically imagining squeezing a ball in their right hand or decrease the tempo by relaxing. The paradigm was split into 9 runs. The first was a calibration run, containing 30 trials in pairs of increase and decrease tempo trials. 

Publication Year: 2018

Creators: Nicoletta Nicolaou, Ian Daly

Contributors: Isil Poyraz Bilgin, James Weaver, Asad Malik, Alexis Kirke, Duncan Williams.

Principal Investigator: Slawomir Nasuto (EP/J003077/1).

Co-Investigator: Eduardo Miranda (EP/J002135/1).

Organisation: University of Reading

Rights-holders: University of Reading

Source: The synthetic generator used to generate the music clips was presented in Williams et al., ‚ÄúAffective Calibration of Musical Feature Sets in an Emotionally Intelligent Music Composition System‚Äù, ACM Trans. Appl. Percept. 14, 3, Article 17 (May 2017), 13 pages. DOI: https://doi.org/10.1145/3059005

3. TERMS OF USE
-----------------

Copyright University of Reading, 2018. This dataset is licensed by the rights-holder(s) under a Creative Commons Attribution 4.0 International Licence: https://creativecommons.org/licenses/by/4.0/.

4. CONTENTS
------------

Zip File listing:
The dataset comprises data from 19 subjects.

The data is provided in BIDS format. The sampling rate is 1 kHz and the EEG corresponding to a music clip is 20 s long (the duration of the clips).


5. METHOD and PROCESSING
--------------------------

This information is available in the following publications:

[1] Daly, I., Ö ìî, Dataset paper, 2018.
[2] Daly, I., Hallowell, J., Hwang, F., Kirke, A., Malik, A., Roesch, E., Weaver, J., Williams, D., Miranda, E. R., Nasuto, S. J., ìChanges in music tempo entrain movement related brain activityî, in Proc. 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC'14), Chicago, Illinois, USA; pp. , 2014a.

[3] Daly, I., Williams, D., Hwang, F., Kirke, A., Malik, A., Roesch, E., Weaver, J., Miranda, E. R., Nasuto, S. J., ìBrain-computer music interfacing for continuous control of musical tempoî, in Proc. 6th International Brain-Computer Interface Conference 2014, Graz, Austria; 2014b

Please cite these references and the reference to the music generator if you use this dataset in your study.

Thank you for your interest in our work.
